choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 24-47: Exited with exit code 1
srun: error: hopper003: tasks 0-23: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 25-48: Exited with exit code 1
srun: error: hopper003: tasks 0-24: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 25-49: Exited with exit code 1
srun: error: hopper003: tasks 0-24: Exited with exit code 1
srun: Job 1810422 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 1810422
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 26-50: Exited with exit code 1
srun: error: hopper003: tasks 0-25: Exited with exit code 1
srun: Job 1810422 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 1810422
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 26-51: Exited with exit code 1
srun: error: hopper003: tasks 0-25: Exited with exit code 1
srun: Job 1810422 step creation temporarily disabled, retrying (Requested nodes are busy)
srun: Step created for job 1810422
srun: error: hopper004: tasks 27-52: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper003: tasks 0-26: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 27-53: Exited with exit code 1
srun: error: hopper003: tasks 0-26: Exited with exit code 1
srun: error: hopper004: tasks 28-54: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper003: tasks 0-27: Exited with exit code 1
srun: error: hopper004: tasks 28-55: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper003: tasks 0-27: Exited with exit code 1
srun: error: hopper004: tasks 29-56: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper003: tasks 0-28: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 29-57: Exited with exit code 1
srun: error: hopper003: tasks 0-28: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 30-58: Exited with exit code 1
srun: error: hopper003: tasks 0-29: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 30-59: Exited with exit code 1
srun: error: hopper003: tasks 0-29: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 31-60: Exited with exit code 1
srun: error: hopper003: tasks 0-30: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper003: tasks 0-30: Exited with exit code 1
srun: error: hopper004: tasks 31-61: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 32-62: Exited with exit code 1
srun: error: hopper003: tasks 0-31: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 32-63: Exited with exit code 1
srun: error: hopper003: tasks 0-31: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 24-47: Exited with exit code 1
srun: error: hopper004: tasks 0-23: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 25-48: Exited with exit code 1
srun: error: hopper004: tasks 0-24: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 25-49: Exited with exit code 1
srun: error: hopper004: tasks 0-24: Exited with exit code 1
srun: error: hopper005: tasks 26-50: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 0-25: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 26-51: Exited with exit code 1
srun: error: hopper004: tasks 0-25: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 27-52: Exited with exit code 1
srun: error: hopper004: tasks 0-26: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 27-53: Exited with exit code 1
srun: error: hopper004: tasks 0-26: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 28-54: Exited with exit code 1
srun: error: hopper004: tasks 0-27: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 28-55: Exited with exit code 1
srun: error: hopper004: tasks 0-27: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 29-56: Exited with exit code 1
srun: error: hopper004: tasks 0-28: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 29-57: Exited with exit code 1
srun: error: hopper004: tasks 0-28: Exited with exit code 1
srun: error: hopper005: tasks 30-58: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 0-29: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 0-29: Exited with exit code 1
srun: error: hopper005: tasks 30-59: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 31-60: Exited with exit code 1
srun: error: hopper004: tasks 0-30: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 31-61: Exited with exit code 1
srun: error: hopper004: tasks 0-30: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 32-62: Exited with exit code 1
srun: error: hopper004: tasks 0-31: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 32-63: Exited with exit code 1
srun: error: hopper004: tasks 0-31: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 0-23: Exited with exit code 1
srun: error: hopper005: tasks 24-47: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 25-48: Exited with exit code 1
srun: error: hopper004: tasks 0-24: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 25-49: Exited with exit code 1
srun: error: hopper004: tasks 0-24: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 0-25: Exited with exit code 1
srun: error: hopper005: tasks 26-50: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 26-51: Exited with exit code 1
srun: error: hopper004: tasks 0-25: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 27-52: Exited with exit code 1
srun: error: hopper004: tasks 0-26: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 27-53: Exited with exit code 1
srun: error: hopper004: tasks 0-26: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 28-54: Exited with exit code 1
srun: error: hopper004: tasks 0-27: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 28-55: Exited with exit code 1
srun: error: hopper004: tasks 0-27: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 29-56: Exited with exit code 1
srun: error: hopper004: tasks 0-28: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 29-57: Exited with exit code 1
srun: error: hopper004: tasks 0-28: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 30-58: Exited with exit code 1
srun: error: hopper004: tasks 0-29: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 30-59: Exited with exit code 1
srun: error: hopper004: tasks 0-29: Exited with exit code 1
srun: error: hopper005: tasks 31-60: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper004: tasks 0-30: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 31-61: Exited with exit code 1
srun: error: hopper004: tasks 0-30: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 32-62: Exited with exit code 1
srun: error: hopper004: tasks 0-31: Exited with exit code 1
choose between STANDARD and THORSTEN, exitingsrun: error: hopper005: tasks 32-63: Exited with exit code 1
srun: error: hopper004: tasks 0-31: Exited with exit code 1
[hopper008:3832489] *** An error occurred in MPI_Allreduce
[hopper008:3832489] *** reported by process [2918580224,1]
[hopper008:3832489] *** on communicator MPI_COMM_WORLD
[hopper008:3832489] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3832489] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3832489] ***    and potentially your MPI job)
[hopper009][[44534,0],32][btl_tcp.c:559:mca_btl_tcp_recv_blocking] recv(14) failed: Connection reset by peer (104)
slurmstepd: error: *** STEP 1814006.0 ON hopper008 CANCELLED AT 2023-02-14T23:51:05 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: hopper009: tasks 24-47: Killed
srun: error: hopper008: tasks 0,2-23: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3832914] *** An error occurred in MPI_Allreduce
[hopper008:3832914] *** reported by process [2918580225,1]
[hopper008:3832914] *** on communicator MPI_COMM_WORLD
[hopper008:3832914] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3832914] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3832914] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3699607
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.1 ON hopper008 CANCELLED AT 2023-02-14T23:51:07 ***
srun: error: hopper009: tasks 25-48: Killed
srun: error: hopper008: tasks 0,2-24: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3833350] *** An error occurred in MPI_Allreduce
[hopper008:3833350] *** reported by process [2918580226,1]
[hopper008:3833350] *** on communicator MPI_COMM_WORLD
[hopper008:3833350] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3833350] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3833350] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3700013
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.2 ON hopper008 CANCELLED AT 2023-02-14T23:51:09 ***
srun: error: hopper009: tasks 25-49: Killed
srun: error: hopper008: tasks 0,2-24: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3833786] *** An error occurred in MPI_Allreduce
[hopper008:3833786] *** reported by process [2918580227,1]
[hopper008:3833786] *** on communicator MPI_COMM_WORLD
[hopper008:3833786] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3833786] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3833786] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3700433
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.3 ON hopper008 CANCELLED AT 2023-02-14T23:51:10 ***
srun: error: hopper009: tasks 26-50: Killed
srun: error: hopper008: tasks 0,2-25: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3834237] *** An error occurred in MPI_Allreduce
[hopper008:3834237] *** reported by process [2918580228,1]
[hopper008:3834237] *** on communicator MPI_COMM_WORLD
[hopper008:3834237] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3834237] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3834237] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3700855
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.4 ON hopper008 CANCELLED AT 2023-02-14T23:51:12 ***
srun: error: hopper008: tasks 0,2-25: Killed
srun: error: hopper009: tasks 26-51: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3834699] *** An error occurred in MPI_Allreduce
[hopper008:3834699] *** reported by process [2918580229,1]
[hopper008:3834699] *** on communicator MPI_COMM_WORLD
[hopper008:3834699] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3834699] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3834699] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3701305
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.5 ON hopper008 CANCELLED AT 2023-02-14T23:51:14 ***
srun: error: hopper009: tasks 27-52: Killed
srun: error: hopper008: tasks 0,2-26: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3835166] *** An error occurred in MPI_Allreduce
[hopper008:3835166] *** reported by process [2918580230,1]
[hopper008:3835166] *** on communicator MPI_COMM_WORLD
[hopper008:3835166] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3835166] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3835166] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3701743
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.6 ON hopper008 CANCELLED AT 2023-02-14T23:51:16 ***
srun: error: hopper009: tasks 27-53: Killed
srun: error: hopper008: tasks 0,2-26: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3835632] *** An error occurred in MPI_Allreduce
[hopper008:3835632] *** reported by process [2918580231,1]
[hopper008:3835632] *** on communicator MPI_COMM_WORLD
[hopper008:3835632] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3835632] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3835632] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3702197
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.7 ON hopper008 CANCELLED AT 2023-02-14T23:51:18 ***
srun: error: hopper009: tasks 28-54: Killed
srun: error: hopper008: tasks 0,2-27: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3836114] *** An error occurred in MPI_Allreduce
[hopper008:3836114] *** reported by process [2918580232,1]
[hopper008:3836114] *** on communicator MPI_COMM_WORLD
[hopper008:3836114] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3836114] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3836114] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3702652
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.8 ON hopper008 CANCELLED AT 2023-02-14T23:51:21 ***
srun: error: hopper008: tasks 0,2-27: Killed
srun: error: hopper009: tasks 28-55: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3836598] *** An error occurred in MPI_Allreduce
[hopper008:3836598] *** reported by process [2918580233,1]
[hopper008:3836598] *** on communicator MPI_COMM_WORLD
[hopper008:3836598] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3836598] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3836598] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3703122
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.9 ON hopper008 CANCELLED AT 2023-02-14T23:51:23 ***
srun: error: hopper008: tasks 0,2-28: Killed
srun: error: hopper008: task 1: Exited with exit code 15
srun: error: hopper009: tasks 29-56: Killed
[hopper008:3837097] *** An error occurred in MPI_Allreduce
[hopper008:3837097] *** reported by process [2918580234,1]
[hopper008:3837097] *** on communicator MPI_COMM_WORLD
[hopper008:3837097] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3837097] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3837097] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3703592
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.10 ON hopper008 CANCELLED AT 2023-02-14T23:51:25 ***
srun: error: hopper009: tasks 29-57: Killed
srun: error: hopper008: tasks 0,2-28: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3837595] *** An error occurred in MPI_Allreduce
[hopper008:3837595] *** reported by process [2918580235,1]
[hopper008:3837595] *** on communicator MPI_COMM_WORLD
[hopper008:3837595] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3837595] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3837595] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3704076
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.11 ON hopper008 CANCELLED AT 2023-02-14T23:51:27 ***
srun: error: hopper009: tasks 30-58: Killed
srun: error: hopper008: tasks 0,2-29: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3838109] *** An error occurred in MPI_Allreduce
[hopper008:3838109] *** reported by process [2918580236,1]
[hopper008:3838109] *** on communicator MPI_COMM_WORLD
[hopper008:3838109] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3838109] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3838109] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3704562
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.12 ON hopper008 CANCELLED AT 2023-02-14T23:51:29 ***
srun: error: hopper009: tasks 30-59: Killed
srun: error: hopper008: tasks 0,2-29: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3838623] *** An error occurred in MPI_Allreduce
[hopper008:3838623] *** reported by process [2918580237,1]
[hopper008:3838623] *** on communicator MPI_COMM_WORLD
[hopper008:3838623] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3838623] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3838623] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3705063
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.13 ON hopper008 CANCELLED AT 2023-02-14T23:51:31 ***
srun: error: hopper009: tasks 31-60: Killed
srun: error: hopper008: tasks 0,2-30: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3839153] *** An error occurred in MPI_Allreduce
[hopper008:3839153] *** reported by process [2918580238,1]
[hopper008:3839153] *** on communicator MPI_COMM_WORLD
[hopper008:3839153] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3839153] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3839153] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3705564
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.14 ON hopper008 CANCELLED AT 2023-02-14T23:51:33 ***
srun: error: hopper008: tasks 0,2-30: Killed
srun: error: hopper009: tasks 31-61: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3839684] *** An error occurred in MPI_Allreduce
[hopper008:3839684] *** reported by process [2918580239,1]
[hopper008:3839684] *** on communicator MPI_COMM_WORLD
[hopper008:3839684] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3839684] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3839684] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3706080
  Peer host:  hopper008
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 1814006.15 ON hopper008 CANCELLED AT 2023-02-14T23:51:36 ***
srun: error: hopper009: tasks 32-62: Killed
srun: error: hopper008: tasks 0,2-31: Killed
srun: error: hopper008: task 1: Exited with exit code 15
[hopper008:3840230] *** An error occurred in MPI_Allreduce
[hopper008:3840230] *** reported by process [2918580240,1]
[hopper008:3840230] *** on communicator MPI_COMM_WORLD
[hopper008:3840230] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3840230] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3840230] ***    and potentially your MPI job)
--------------------------------------------------------------------------
WARNING: Open MPI failed to TCP connect to a peer MPI process.  This
should not happen.

Your Open MPI job may now hang or fail.

  Local host: hopper009
  PID:        3706599
  Message:    connect() to 172.16.1.8:1296 failed
  Error:      Operation now in progress (115)
--------------------------------------------------------------------------
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
[hopper009:3706597] *** An error occurred in MPI_Allreduce
[hopper009:3706597] *** reported by process [2918580240,32]
[hopper009:3706597] *** on communicator MPI_COMM_WORLD
[hopper009:3706597] *** MPI_ERR_TRUNCATE: message truncated
[hopper009:3706597] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper009:3706597] ***    and potentially your MPI job)
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3706601
  Peer host:  hopper008
--------------------------------------------------------------------------
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: hopper009
  Local PID:  3706605
  Peer host:  hopper008
--------------------------------------------------------------------------
[hopper008:3840231] *** An error occurred in MPI_Allreduce
[hopper008:3840231] *** reported by process [2918580240,2]
[hopper008:3840231] *** on communicator MPI_COMM_WORLD
[hopper008:3840231] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3840231] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3840231] ***    and potentially your MPI job)
[hopper008:3840233] *** An error occurred in MPI_Allreduce
[hopper008:3840233] *** reported by process [2918580240,4]
[hopper008:3840233] *** on communicator MPI_COMM_WORLD
[hopper008:3840233] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3840233] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3840233] ***    and potentially your MPI job)
[hopper008:3840237] *** An error occurred in MPI_Allreduce
[hopper008:3840237] *** reported by process [2918580240,8]
[hopper008:3840237] *** on communicator MPI_COMM_WORLD
[hopper008:3840237] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3840237] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3840237] ***    and potentially your MPI job)
[hopper008:3840245] *** An error occurred in MPI_Allreduce
[hopper008:3840245] *** reported by process [2918580240,16]
[hopper008:3840245] *** on communicator MPI_COMM_WORLD
[hopper008:3840245] *** MPI_ERR_TRUNCATE: message truncated
[hopper008:3840245] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper008:3840245] ***    and potentially your MPI job)
slurmstepd: error: *** STEP 1814006.16 ON hopper008 CANCELLED AT 2023-02-14T23:51:38 ***
srun: error: hopper009: tasks 32-63: Killed
srun: error: hopper008: tasks 0-1,3-31: Killed
srun: error: hopper008: task 2: Exited with exit code 15
